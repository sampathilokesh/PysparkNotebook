{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6687b3af-c507-43ab-84c8-3d30ea3897b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n| id|  name|\n+---+------+\n|  1|lokesh|\n|  2|  gopi|\n|  3|  hari|\n|  4|  ravi|\n+---+------+\n\n+---+-----+\n| id| DEPT|\n+---+-----+\n|  1|   IT|\n|  2|   HR|\n|  3|SALES|\n|  5|  SEC|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# joins \n",
    "data1=[(1,'lokesh'),(2,'gopi'),(3,'hari'),(4,'ravi')]\n",
    "schema1=['id','name']\n",
    "df1=spark.createDataFrame(data1,schema1)\n",
    "df1.show()\n",
    "data2=[(1,'IT'),(2,'HR'),(3,'SALES'),(5,'SEC')]\n",
    "schema2=['id','DEPT']\n",
    "df2=spark.createDataFrame(data2,schema2)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96da4d79-abfa-48d3-9f4a-07bd4055a7a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+-----+\n| id|  name| id| DEPT|\n+---+------+---+-----+\n|  1|lokesh|  1|   IT|\n|  2|  gopi|  2|   HR|\n|  3|  hari|  3|SALES|\n+---+------+---+-----+\n\n+---+------+----+-----+\n| id|  name|  id| DEPT|\n+---+------+----+-----+\n|  1|lokesh|   1|   IT|\n|  2|  gopi|   2|   HR|\n|  3|  hari|   3|SALES|\n|  4|  ravi|null| null|\n+---+------+----+-----+\n\n+----+------+---+-----+\n|  id|  name| id| DEPT|\n+----+------+---+-----+\n|   1|lokesh|  1|   IT|\n|   2|  gopi|  2|   HR|\n|   3|  hari|  3|SALES|\n|null|  null|  5|  SEC|\n+----+------+---+-----+\n\n+----+------+----+-----+\n|  id|  name|  id| DEPT|\n+----+------+----+-----+\n|   1|lokesh|   1|   IT|\n|   2|  gopi|   2|   HR|\n|   3|  hari|   3|SALES|\n|   4|  ravi|null| null|\n|null|  null|   5|  SEC|\n+----+------+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,df1.id==df2.id,'inner').show()\n",
    "df1.join(df2,df1.id==df2.id,'left').show()\n",
    "df1.join(df2,df1.id==df2.id,'right').show()\n",
    "df1.join(df2,df1.id==df2.id,'outer').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46b9fd8a-6798-4864-a9b6-84dbc17f8efa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n| id|  name|\n+---+------+\n|  1|lokesh|\n|  2|  gopi|\n|  3|  hari|\n+---+------+\n\n+---+----+\n| id|name|\n+---+----+\n|  4|ravi|\n+---+----+\n\n"
     ]
    }
   ],
   "source": [
    "#left semi,left anti\n",
    "# leftsami -> it is like inner join but gives you only matching rows those are only from left table\n",
    "# leftanti -> it is like inner join but gives you only unmatched rows those are only from left table\n",
    "# self - > used to join on dataframe itself\n",
    "df1.join(df2,df1.id==df2.id,'leftsemi').show()\n",
    "df1.join(df2,df1.id==df2.id,'leftanti').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d04cd27b-9baf-4ea7-92b7-0957127d20f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n| id|  name|mgrid|\n+---+------+-----+\n|  1|lokesh|    0|\n|  2|  gopi|    1|\n|  3|  hari|    2|\n|  4|  ravi|    1|\n+---+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# self\n",
    "data=[(1,'lokesh',0),(2,'gopi',1),(3,'hari',2),(4,'ravi',1)]\n",
    "schema=['id','name','mgrid']\n",
    "df1=spark.createDataFrame(data,schema)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a06da51d-ce0d-42a7-9164-01d999d2cf97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n|empname|mgrname|\n+-------+-------+\n| lokesh|   null|\n|   gopi| lokesh|\n|   hari|   gopi|\n|   ravi| lokesh|\n+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df1.alias('emp').join(df1.alias('mgr'),col('emp.mgrid')==col('mgr.id'),'left').select(col('emp.name').alias('empname'),col('mgr.name').alias('mgrname')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9619cf55-5bcb-4b9d-979e-c138e79d0ff5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+\n| id|  name|gender| dept|\n+---+------+------+-----+\n|  1|lokesh|  male|   IT|\n|  2|  gopi|  male|SALES|\n|  3|  hari|  male|   HR|\n|  4|  ravi|  male|   IT|\n|  5|   sai|female|   IT|\n|  6|  navi|female|   HR|\n+---+------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data1=[(1,'lokesh','male','IT'),\\\n",
    "    (2,'gopi','male','SALES'),\\\n",
    "    (3,'hari','male','HR'),\\\n",
    "    (4,'ravi', 'male','IT'),\\\n",
    "    (5,'sai','female','IT'),\\\n",
    "    (6,'navi','female','HR')]\n",
    "schema1=['id','name','gender','dept']\n",
    "df1=spark.createDataFrame(data1,schema1)\n",
    "df1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf5e6312-5668-4840-9ff3-57f646cd8be1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6a90f19-1c5b-40e3-8962-a214acb40f32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n| dept|gender|count|\n+-----+------+-----+\n|   IT|  male|    2|\n|SALES|  male|    1|\n|   HR|  male|    1|\n|   IT|female|    1|\n|   HR|female|    1|\n+-----+------+-----+\n\n+-----+------+----+\n| dept|female|male|\n+-----+------+----+\n|   HR|     1|   1|\n|SALES|  null|   1|\n|   IT|     1|   2|\n+-----+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "# pivot -> it used to generate multiple columns of distinct values in a single column \n",
    "# it is a aggregte function we can also perform aggregate function on it\n",
    "df1.groupBy('dept','gender').count().show()\n",
    "df1.groupBy('dept').pivot('gender').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f72e2bc-1a6f-4c96-8058-92c16dc291fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>gender</th><th>dept</th><th>dummy</th></tr></thead><tbody><tr><td>1</td><td>lokesh</td><td>male</td><td>IT</td><td>Dummy</td></tr><tr><td>2</td><td>gopi</td><td>male</td><td>SALES</td><td>Dummy</td></tr><tr><td>3</td><td>hari</td><td>male</td><td>HR</td><td>Dummy</td></tr><tr><td>4</td><td>ravi</td><td>male</td><td>IT</td><td>Dummy</td></tr><tr><td>5</td><td>sai</td><td>female</td><td>IT</td><td>Dummy</td></tr><tr><td>6</td><td>navi</td><td>female</td><td>HR</td><td>Dummy</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "lokesh",
         "male",
         "IT",
         "Dummy"
        ],
        [
         2,
         "gopi",
         "male",
         "SALES",
         "Dummy"
        ],
        [
         3,
         "hari",
         "male",
         "HR",
         "Dummy"
        ],
        [
         4,
         "ravi",
         "male",
         "IT",
         "Dummy"
        ],
        [
         5,
         "sai",
         "female",
         "IT",
         "Dummy"
        ],
        [
         6,
         "navi",
         "female",
         "HR",
         "Dummy"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dept",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dummy",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2=df1.withColumn('dummy',lit('Dummy'))\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ada2cc76-7a6a-4f6b-aeb0-239a6a252fde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n| HR| IT|SALES|\n+---+---+-----+\n|  2|  3|    1|\n+---+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy('Dummy').pivot('dept').count().select(col('HR'),col('IT'),col('SALES')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c95e88dd-99df-442a-90ed-5a1a72a5fd15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-212519517037227>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m help(\u001B[43mpivot\u001B[49m())\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'pivot' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-212519517037227>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m help(\u001B[43mpivot\u001B[49m())\n\n\u001B[0;31mNameError\u001B[0m: name 'pivot' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'pivot' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d20c4dd-b9bb-4fb1-a1f6-1e019444547a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+\n| dept|female|male|\n+-----+------+----+\n|   HR|     1|   1|\n|SALES|  null|   1|\n|   IT|     1|   2|\n+-----+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "df=df1.groupBy('dept').pivot('gender').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5debb787-5a0c-4254-a0af-69fefbf6e6a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# unppivot -> used to convert multiplecolumns into single column as rows \n",
    "# but in pyspark there is no unpivot function but we can achieve it by usung stack() function\n",
    "from pyspark.sql.functions import expr\n",
    "# in expr() we write code in string but it execute as a code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26aabc26-1758-47fb-9a02-9c421844ac7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-212519517037230>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m unpivotdf\u001B[38;5;241m=\u001B[39m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdept\u001B[39m\u001B[38;5;124m'\u001B[39m,expr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstack(2,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mM\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,male,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mF\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,female) as (\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgender\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,count)\u001B[39m\u001B[38;5;124m\"\u001B[39m) )\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'select'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-212519517037230>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m unpivotdf\u001B[38;5;241m=\u001B[39m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdept\u001B[39m\u001B[38;5;124m'\u001B[39m,expr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstack(2,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mM\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,male,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mF\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,female) as (\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgender\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,count)\u001B[39m\u001B[38;5;124m\"\u001B[39m) )\n\n\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'select'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'NoneType' object has no attribute 'select'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "unpivotdf=df.select('dept',expr(\"stack(2,'M',male,'F',female) as ('gender',count)\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97941ead-346b-4b8f-ad03-f1afce7ede04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+\n| id|  name|gender|dept|\n+---+------+------+----+\n|  1|lokesh|  male|  IT|\n|  2|  gopi|  male|null|\n|  3|  hari|  male|  HR|\n|  4|  ravi|  male|null|\n|  5|   sai|female|  IT|\n|  6|  navi|  null|  HR|\n+---+------+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "data=[(1,'lokesh','male','IT'),\\\n",
    "    (2,'gopi','male',None),\\\n",
    "    (3,'hari','male','HR'),\\\n",
    "    (4,'ravi', 'male',None),\\\n",
    "    (5,'sai','female','IT'),\\\n",
    "    (6,'navi',None,'HR')]\n",
    "schema=['id','name','gender','dept']\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8f9f98d-89b6-4f90-8d50-2fe85361f7de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----------+\n| id|  name|    gender|      dept|\n+---+------+----------+----------+\n|  1|lokesh|      male|        IT|\n|  2|  gopi|      male|processing|\n|  3|  hari|      male|        HR|\n|  4|  ravi|      male|processing|\n|  5|   sai|    female|        IT|\n|  6|  navi|processing|        HR|\n+---+------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# fill & fillna used to replace your null values with values u want \n",
    "df.fillna('processing').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45b0bd91-8fb9-4958-ace7-97f685eaa9ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----------+\n| id|  name|gender|      dept|\n+---+------+------+----------+\n|  1|lokesh|  male|        IT|\n|  2|  gopi|  male|processing|\n|  3|  hari|  male|        HR|\n|  4|  ravi|  male|processing|\n|  5|   sai|female|        IT|\n|  6|  navi|  null|        HR|\n+---+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.fillna('processing',['dept']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aef818c4-25f9-42dc-bf3c-01152cb7c7a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+----+\n| id|  name| gender|dept|\n+---+------+-------+----+\n|  1|lokesh|   male|  IT|\n|  2|  gopi|   male|null|\n|  3|  hari|   male|  HR|\n|  4|  ravi|   male|null|\n|  5|   sai| female|  IT|\n|  6|  navi|pending|  HR|\n+---+------+-------+----+\n\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('pending',['gender']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98d0e027-f42b-4958-9248-708781ec9bb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pypark-4 Joins",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
