{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9005238e-c4ab-42f1-b1d0-68788011bfc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "857bed04-8a55-42fe-96e6-15f936be4f6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>company</th><th>domain</th></tr></thead><tbody><tr><td>1</td><td>lokesh</td><td>HCL</td><td>RPA</td></tr><tr><td>2</td><td>GOPAL</td><td>TCS</td><td>MEDICAL</td></tr><tr><td>3</td><td>SUHAIL</td><td>HCL</td><td>RPA</td></tr><tr><td>4</td><td>ESWAR</td><td>INFOSYS</td><td>APPS</td></tr><tr><td>5</td><td>RAMU</td><td>HCL</td><td>RPA</td></tr><tr><td>6</td><td>MANOJ</td><td>TCS</td><td>APPS</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "lokesh",
         "HCL",
         "RPA"
        ],
        [
         2,
         "GOPAL",
         "TCS",
         "MEDICAL"
        ],
        [
         3,
         "SUHAIL",
         "HCL",
         "RPA"
        ],
        [
         4,
         "ESWAR",
         "INFOSYS",
         "APPS"
        ],
        [
         5,
         "RAMU",
         "HCL",
         "RPA"
        ],
        [
         6,
         "MANOJ",
         "TCS",
         "APPS"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "company",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "domain",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "schema=StructType([\\\n",
    "    StructField('id',IntegerType()),\\\n",
    "    StructField('name',StringType()),\\\n",
    "    StructField('company',StringType()),\\\n",
    "    StructField('domain',StringType())])\n",
    "row1=Row(1,'lokesh','HCL','RPA')\n",
    "row2=Row(2,'GOPAL','TCS','MEDICAL')\n",
    "row3=Row(3,'SUHAIL','HCL','RPA')\n",
    "row4=Row(4,'ESWAR','INFOSYS','APPS')\n",
    "row5=Row(5,'RAMU','HCL','RPA')\n",
    "row6=Row(6,'MANOJ','TCS','APPS')\n",
    "data=[row1,row2,row3,row4,row5,row6]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eebdfdd7-98af-4541-93f9-c66b232c05c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.parquet('/FileStore/json/employees',mode='overwrite',partitionBy=['company','domain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a6597c3-f55c-4931-a4a2-f500d343080a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+-------+\n| id|  name|company| domain|\n+---+------+-------+-------+\n|  1|lokesh|    HCL|    RPA|\n|  3|SUHAIL|    HCL|    RPA|\n|  4| ESWAR|INFOSYS|   APPS|\n|  2| GOPAL|    TCS|MEDICAL|\n|  6| MANOJ|    TCS|   APPS|\n|  5|  RAMU|    HCL|    RPA|\n+---+------+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df1=spark.read.parquet('/FileStore/json/employees').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6eaf3a2-a791-4c71-91e7-ec7df49904bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|domain|\n+---+------+------+\n|  1|lokesh|   RPA|\n|  3|SUHAIL|   RPA|\n|  5|  RAMU|   RPA|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet('/FileStore/json/employees/company=HCL').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94382c88-71f8-452c-a4b1-d699fc147540",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n| id| name|\n+---+-----+\n|  2|GOPAL|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet('/FileStore/json/employees/company=TCS/domain=MEDICAL').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feb96b3a-40ed-4761-be5f-c34f6fc24fe0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert json string to MapType or StructType\n",
    "#   supose we have string in json format , then we cannot access keys or values from it because it is in string format so that we should convert json string to MapType pr StructType for that we use from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "614bacb3-f79f-473a-b9d5-e27aa2edbccf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StringType,MapType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c26dbb4c-83aa-43a6-8f9a-47b34408cb8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- marks: string (nullable = true)\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>marks</th></tr></thead><tbody><tr><td>1</td><td>lokesh</td><td>{\"telugu\":\"25\",\"english\":\"24\"}</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "lokesh",
         "{\"telugu\":\"25\",\"english\":\"24\"}"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "marks",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data=[(1,'lokesh','{\"telugu\":\"25\",\"english\":\"24\"}')]\n",
    "schema=['id','name','marks']\n",
    "dfr=spark.createDataFrame(data,schema)\n",
    "dfr.printSchema()\n",
    "display(dfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb5d42c9-2842-4882-a5e3-0092920a10b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- Marks: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+---+------+-----------------------------+\n|id |name  |Marks                        |\n+---+------+-----------------------------+\n|1  |lokesh|{telugu -> 25, english -> 24}|\n+---+------+-----------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "Mapschema=MapType(keyType=StringType(),valueType=StringType())\n",
    "dfr1=dfr.withColumn('Marks',from_json(dfr.marks,Mapschema))\n",
    "dfr1.printSchema()\n",
    "dfr1.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea61c7e5-b4d7-4b9e-89e2-e64ce64c6a18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+------+\n| id|  name|               Marks|Telugu|\n+---+------+--------------------+------+\n|  1|lokesh|{telugu -> 25, en...|    25|\n+---+------+--------------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "dfr1.withColumn('Telugu',dfr1.Marks.telugu).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "673902ec-a6f2-4e6d-a470-98d6e8051a04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- marks: string (nullable = true)\n |-- structschema(marks): struct (nullable = true)\n |    |-- telugu: string (nullable = true)\n |    |-- english: string (nullable = true)\n\n+---+------+--------------------+-------------------+\n| id|  name|               marks|structschema(marks)|\n+---+------+--------------------+-------------------+\n|  1|lokesh|{\"telugu\":\"25\",\"e...|           {25, 24}|\n+---+------+--------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# convert into struct type\n",
    "structschema=StructType([\\\n",
    "    StructField('telugu',StringType()),\\\n",
    "    StructField('english',StringType())])\n",
    "dfr2=dfr.withColumn('structschema(marks)',from_json(dfr.marks,structschema))\n",
    "dfr2.printSchema()\n",
    "dfr2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42bd9d96-c724-4c0e-90ae-2dec85d40606",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aa57d9d-3912-4aa6-989a-2ce2c3819a6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+-------------------+------+\n| id|  name|               marks|structschema(marks)|telugu|\n+---+------+--------------------+-------------------+------+\n|  1|lokesh|{\"telugu\":\"25\",\"e...|           {25, 24}|    25|\n+---+------+--------------------+-------------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "dfr2.withColumn('telugu',col('structschema(marks)').telugu).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a46aa026-e5db-4bc0-a63c-bd1221d6827e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- Marks: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "dfr1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fb773b0-6299-4b7a-acf2-ded1c89a6647",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcb509c1-f694-4b9e-bb50-f31888557303",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- Marks: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n |-- jsonmarks: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# we try to convert this into json string bu using to_json\n",
    "dfr3=dfr1.withColumn('jsonmarks',to_json(dfr1.Marks))\n",
    "dfr3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74a1e675-d49d-4e04-8a3f-c7fea0f47647",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# json_tuple used to query or extract fields or properties from json string\n",
    "from pyspark.sql.functions import json_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e7b374a-5fd5-42ee-a9a8-ed345eb9e108",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------+\n| id|  name|Telugu|English|\n+---+------+------+-------+\n|  1|lokesh|    25|     24|\n+---+------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df5=dfr.select(dfr.id,dfr.name,json_tuple(dfr.marks,'telugu','english').alias('Telugu','English'))\n",
    "df5.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cc7f4b3-840f-40b5-91e6-332b630fdea5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we cam acces json objects from json string by using get_json_object easily with out doing above operations\n",
    "from pyspark.sql.functions import get_json_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93a2361e-96a8-4085-b099-6698082a1b50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n|  name|Telugu|\n+------+------+\n|lokesh|    25|\n+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "dfr.select('name',get_json_object('marks','$.telugu').alias('Telugu')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27c2cb67-821d-42f7-9bff-27e23d83e06c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark-7 partitionBy",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
