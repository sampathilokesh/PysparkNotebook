{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dad6e8c6-a3df-453e-919f-7fca4a3a0beb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # udf \n",
    "# it is called user defined function , we can wrie one logic using python or spark then we can register that logic with function name by using udf() then we can use it like normal functions to perform that logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aa95165-0b1f-4f63-989c-bb8cd0543627",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>salary</th><th>bonus</th></tr></thead><tbody><tr><td>1</td><td>lokesh</td><td>2000</td><td>100</td></tr><tr><td>2</td><td>gopal</td><td>3000</td><td>200</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "lokesh",
         2000,
         100
        ],
        [
         2,
         "gopal",
         3000,
         200
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "bonus",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data=[(1,'lokesh',2000,100),(2,'gopal',3000,200)]\n",
    "schema =['id','name','salary','bonus']\n",
    "df=spark.createDataFrame(data,schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85632b82-1e9a-44b7-a29a-caeca6c7eba0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2a5614a-29f3-4d3a-bad4-c04d72973fff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def total(s,b) :\n",
    "    return s+b\n",
    "\n",
    "# register by using udf with functon TotalPay\n",
    "TotalPay=udf(lambda s,b : total(s,b),IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b2f52ac-6500-407c-ba10-4e4f204dc8c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+------------+\n| id|  name|salary|bonus|Total Salary|\n+---+------+------+-----+------------+\n|  1|lokesh|  2000|  100|        2100|\n|  2| gopal|  3000|  200|        3200|\n+---+------+------+-----+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('Total Salary',TotalPay(df.salary,df.bonus)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fa40380-f141-4dbf-a1e6-e903bff7f036",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# register function using another method\n",
    "@udf(returntype=IntegerType)\n",
    "    def total(s,b) :\n",
    "      return s+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17f692ca-c037-452a-a15a-f5d1f1df7006",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+------------+\n| id|  name|salary|bonus|Total Salary|\n+---+------+------+-----+------------+\n|  1|lokesh|  2000|  100|        2100|\n|  2| gopal|  3000|  200|        3200|\n+---+------+------+-----+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('Total Salary',total(df.salary,df.bonus)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abe71b35-ee70-4672-b077-0e0ff9a641d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('students')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0af18f72-fd33-401c-974e-de431fe10eb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: <function __main__.total(s, b)>"
     ]
    }
   ],
   "source": [
    "# to use udf function name in sql we register like\n",
    "def total(s,b) :\n",
    "      return s+b\n",
    "\n",
    "spark.udf.register('TotalPay',total,IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09f1f71e-0162-468a-9616-6e8cc2757e74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>salary</th><th>bonus</th><th>TotalSalary</th></tr></thead><tbody><tr><td>1</td><td>lokesh</td><td>2000</td><td>100</td><td>2100</td></tr><tr><td>2</td><td>gopal</td><td>3000</td><td>200</td><td>3200</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "lokesh",
         2000,
         100,
         2100
        ],
        [
         2,
         "gopal",
         3000,
         200,
         3200
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "bonus",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "TotalSalary",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select *,TotalPay(salary,bonus) as TotalSalary from students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ae2a741-3082-42bd-a63b-f35ed6dd770c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# resilent distributed dataset(RDD)\n",
    "# it is called collection og objects similar to python , the advantage Of RDD  is \n",
    "# it it immutable \n",
    "# fault tolerance \n",
    "# parallelizing\n",
    "# partitioning \n",
    "# in memory processing\n",
    "#it is fundamantal data struccture in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab63bdc8-c708-4bc7-b497-04fcc5aaee24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we create RDD by ug sparkcontext instance and parallelize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07a1fcf0-b179-4d93-a493-d5d26fea2b8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'lokesh', 2000, 100), (2, 'gopal', 3000, 200)]\n"
     ]
    }
   ],
   "source": [
    "data=[(1,'lokesh',2000,100),(2,'gopal',3000,200)]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a39d602-7d3a-4a43-a4f9-30336ff6ddc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+\n| id|  name|salary|bonus|\n+---+------+------+-----+\n|  1|lokesh|  2000|  100|\n|  2| gopal|  3000|  200|\n+---+------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# to convert rdd into dataframe\n",
    "schema =['id','name','salary','bonus']\n",
    "\n",
    "df=rdd.toDF(schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d85ec56-1585-496c-9c51-7ead497e5f64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+\n| id|  name|salary|bonus|\n+---+------+------+-----+\n|  1|lokesh|  2000|  100|\n|  2| gopal|  3000|  200|\n+---+------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# we can also create by using spark data frame\n",
    "df1=spark.createDataFrame(rdd,schema)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7eee8c8f-66dc-41cc-a7b5-faa6d8a69a92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# map()\n",
    "# it's RDD transformation , used to peroform transormation functions on RDD to alter rows or anything, we use lambda fuction to take element from rdd and perform operations on  it. \n",
    "# in dataframe we don't have map() function to perform for that we need create daaframe first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c03275-d44c-4a33-9b4c-eebf2c7748fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'lokesh', 2000, 100, 2100), (2, 'gopal', 3000, 200, 3200)]\n"
     ]
    }
   ],
   "source": [
    "rdd2=rdd.map(lambda x : x+(x[2]+x[3],))\n",
    "print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43855c14-3405-4569-9794-6b636cdeafdb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+------------+\n| id|  name|salary|bonus|total salary|\n+---+------+------+-----+------------+\n|  1|lokesh|  2000|  100|        2100|\n|  2| gopal|  3000|  200|        3200|\n+---+------+------+-----+------------+\n\n"
     ]
    }
   ],
   "source": [
    "rdd3=df1.rdd.map(lambda x : x+(x[2]+x[3],))\n",
    "df2=rdd3.toDF(['id','name','salary','bonus','total salary'])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b5a2af3-43ec-48c2-ae8f-9d76369a0dbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#flatMap \n",
    "#it is used to flatten an array , flatten means distributed array elements in to individual rows like explodse in dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4c13408-6349-4be6-bcd1-3c750155c4d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SAMPATHI', 'LOKESH']\n['RAVI', 'GOPAL']\n"
     ]
    }
   ],
   "source": [
    "# suppose we have some data if we try to split or perofoem any operation on it it will return you array\n",
    "data=['SAMPATHI LOKESH','RAVI GOPAL']\n",
    "Rdd = spark.sparkContext.parallelize(data)\n",
    "Rdd2=Rdd.map(lambda x : x.split(' '))\n",
    "for item in Rdd2.collect():\n",
    "    print(item)\n",
    "# it will return you list by splitting elemnts by space in array but if tou want to flatten you will use flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60112a5e-2b63-4ee6-847b-4fff370b50f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPATHI\nLOKESH\nRAVI\nGOPAL\n"
     ]
    }
   ],
   "source": [
    "Rdd3=Rdd.flatMap(lambda x: x.split(' '))\n",
    "for item in Rdd3.collect():\n",
    "    print(item)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5501db8d-59b0-4646-9484-a429aaf93e01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2421153939769679,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark-6 fuctions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
