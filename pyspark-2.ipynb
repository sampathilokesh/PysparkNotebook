{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "580ab47d-a0d6-4f48-9851-b07d7b295550",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n| id|name|age|\n+---+----+---+\n|  1|loki| 22|\n|  2|gopi| 25|\n+---+----+---+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- age: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# usage of with column : to change value, to change data type, to create new column\n",
    "data=[(1,'loki','22'),(2,'gopi',25)]\n",
    "schema=['id','name','age']\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cd46697-3057-4e9c-96c0-aadbcbc25bac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "# lit is used to pass value to clumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76867384-a41b-441f-84f1-6f175b0dca09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# to change data type of column using with column\n",
    "# IT WILL CREATE NEW DATAFRAME , in spark when we do any altration or udation it's not going to change\n",
    "df1=df.withColumn('Age',col('age').cast('Integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e84b366c-e8a3-470d-afaa-8ec80c08c483",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n| id|name|Age|\n+---+----+---+\n|  1|loki| 44|\n|  2|gopi| 50|\n+---+----+---+\n\n"
     ]
    }
   ],
   "source": [
    "# To update value in a column\n",
    "#df2=df1.withColumn('Age',df1.Age+1) # first parameter is new column u want to create , second parm sholud be  column type\n",
    "df2=df1.withColumn('Age',col('Age')*2)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c6e8d09-7779-488a-a188-e68b9c234ca7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+----------+\n| id|name|Age|  joindate|\n+---+----+---+----------+\n|  1|loki| 22|2020-10-10|\n|  2|gopi| 25|2020-10-10|\n+---+----+---+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# it will create new column beacuse there is no column with joindate\n",
    "df3=df1.withColumn('joindate',to_date(lit('2020-10-10'),'yyyy-MM-dd')) #to_date('value of date in str format', the format of that date) we conveted string to date value\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02985ad7-9550-4a74-95fc-de302a047840",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+----------+\n| id|name|Age|  joindate|\n+---+----+---+----------+\n|  1|loki| 22|2020/10/10|\n|  2|gopi| 25|2020/10/10|\n+---+----+---+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# date format is used to convert one fornat of data int oanother format\n",
    "df3.withColumn('joindate',date_format(col('joindate'),'yyyy/MM/dd')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4631e1b-3879-4e71-ae53-bfd985530213",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method withColumnRenamed in module pyspark.sql.dataframe:\n\nwithColumnRenamed(existing: str, new: str) -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n    Returns a new :class:`DataFrame` by renaming an existing column.\n    This is a no-op if the schema doesn't contain the given column name.\n    \n    .. versionadded:: 1.3.0\n    \n    .. versionchanged:: 3.4.0\n        Support Spark Connect.\n    \n    Parameters\n    ----------\n    existing : str\n        string, name of the existing column to rename.\n    new : str\n        string, new name of the column.\n    \n    Returns\n    -------\n    :class:`DataFrame`\n        DataFrame with renamed column.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n    >>> df.withColumnRenamed('age', 'age2').show()\n    +----+-----+\n    |age2| name|\n    +----+-----+\n    |   2|Alice|\n    |   5|  Bob|\n    +----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# rename the existing column\n",
    "help(df3.withColumnRenamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "102027cd-a9d3-4af1-b0df-1b5a90c67be6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+-----------+\n| id|name|Age|JoiningDate|\n+---+----+---+-----------+\n|  1|loki| 22| 2020-10-10|\n|  2|gopi| 25| 2020-10-10|\n+---+----+---+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df4=df3.withColumnRenamed('joindate','JoiningDate')\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33006120-f953-4fc2-98c1-e73263bbc6a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>age</th></tr></thead><tbody><tr><td>1</td><td>List(sampathi , lokesh)</td><td>25</td></tr><tr><td>2</td><td>List(sampathi, gopal)</td><td>23</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         [
          "sampathi ",
          "lokesh"
         ],
         25
        ],
        [
         2,
         [
          "sampathi",
          "gopal"
         ],
         23
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"firstname\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"lastname\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- age: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# CREATE COMPLEX COLUMNS USING StructType & StructField\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "data1=[\\\n",
    "    (1,('sampathi ','lokesh'),25),\\\n",
    "    (2,('sampathi','gopal'),23)   ]\n",
    "nestedschema=StructType([StructField('firstname',StringType()),\\\n",
    "                    StructField('lastname',StringType())\n",
    "                    ])\n",
    "schema=StructType([StructField('id',IntegerType()),\\\n",
    "                    StructField('name',nestedschema),\\\n",
    "                    StructField('age',IntegerType())])\n",
    "\n",
    "df=spark.createDataFrame(data1,schema)\n",
    "display(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "421df2d0-3b46-4f91-a4cd-b1df124ef3fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>age</th></tr></thead><tbody><tr><td>1</td><td>List(sampathi , lokesh)</td><td>25</td></tr><tr><td>2</td><td>List(sampathi, gopal)</td><td>23</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         [
          "sampathi ",
          "lokesh"
         ],
         25
        ],
        [
         2,
         [
          "sampathi",
          "gopal"
         ],
         23
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using ArrayType column & operations on it\n",
    "    # accessing using index\n",
    "    # combining values into single array\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.functions import col,array\n",
    "data2=[\\\n",
    "    (1,['sampathi ','lokesh'],25),\\\n",
    "    (2,['sampathi','gopal'],23)   ]\n",
    "schema2=StructType([StructField('id',IntegerType()),\\\n",
    "                    StructField('name',ArrayType(StringType())),\\\n",
    "                    StructField('age',IntegerType())])\n",
    "df1=spark.createDataFrame(data2,schema2)\n",
    "display(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25842bd2-bd79-456e-936d-0f2213f9e20e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+---+---------+\n| id|               name|age|firstname|\n+---+-------------------+---+---------+\n|  1|[sampathi , lokesh]| 25|sampathi |\n|  2|  [sampathi, gopal]| 23| sampathi|\n+---+-------------------+---+---------+\n\n+---+-------------------+---+--------+\n| id|               name|age|lastname|\n+---+-------------------+---+--------+\n|  1|[sampathi , lokesh]| 25|  lokesh|\n|  2|  [sampathi, gopal]| 23|   gopal|\n+---+-------------------+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#access data using index\n",
    "df1.withColumn('firstname',df1.name[0]).show()\n",
    "df1.withColumn('lastname',df1.name[1]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b53e77a1-afc2-4eb8-970e-de0c2eb3dfcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>age</th><th>firstname</th><th>lastname</th></tr></thead><tbody><tr><td>1</td><td>List(sampathi , lokesh)</td><td>25</td><td>sampathi </td><td>lokesh</td></tr><tr><td>2</td><td>List(sampathi, gopal)</td><td>23</td><td>sampathi</td><td>gopal</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         [
          "sampathi ",
          "lokesh"
         ],
         25,
         "sampathi ",
         "lokesh"
        ],
        [
         2,
         [
          "sampathi",
          "gopal"
         ],
         23,
         "sampathi",
         "gopal"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "firstname",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "lastname",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1=df1.withColumn('firstname',df1.name[0]).withColumn('lastname',df1.name[1])\n",
    "display(df1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f7e9648-f6fd-4303-80a1-0e28cda4fd9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>firstname</th><th>lastname</th></tr></thead><tbody><tr><td>samapthi</td><td>lokesh</td></tr><tr><td>sampathi</td><td>gopal</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "samapthi",
         "lokesh"
        ],
        [
         "sampathi",
         "gopal"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "firstname",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "lastname",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data3=[('samapthi','lokesh'),\\\n",
    "    ('sampathi','gopal')]\n",
    "schema3=['firstname','lastname']\n",
    "df2=spark.createDataFrame(data3,schema3)\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e801f0c9-d478-47bf-9ada-e18c0e1e5301",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+\n|firstname|lastname|          fullname|\n+---------+--------+------------------+\n| samapthi|  lokesh|[samapthi, lokesh]|\n| sampathi|   gopal| [sampathi, gopal]|\n+---------+--------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#combining diiferent values into single array\n",
    "df2.withColumn('fullname',array(df2.firstname,df2.lastname)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3909f96-c8ec-4bae-9b3d-fcac23108f47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#functions used in array type columns\n",
    "    # explode\n",
    "    # split\n",
    "    # array\n",
    "    # array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b7bf3ea-9d06-4217-b2d5-6d12ef593aa6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode,col,split,array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8e8d86b-c6b7-4499-84b9-4d9aeb9f853c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------+------+\n|name  |skills                |skill |\n+------+----------------------+------+\n|lokesh|[java, python, sql]   |java  |\n|lokesh|[java, python, sql]   |python|\n|lokesh|[java, python, sql]   |sql   |\n|gopi  |[python, dotnet, dbms]|python|\n|gopi  |[python, dotnet, dbms]|dotnet|\n|gopi  |[python, dotnet, dbms]|dbms  |\n+------+----------------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# explode used to devide each elment of array into new rows of a array column\n",
    "data=[('lokesh',['java','python','sql']),('gopi',['python','dotnet','dbms'])]\n",
    "schema=['name','skills']\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df=df.withColumn('skill',explode(col('skills')))\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59276d72-aa3d-41aa-8a1f-57328b9aceb9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+----------------------+\n|name  |skills            |skills in array       |\n+------+------------------+----------------------+\n|lokesh|java,python,sql   |[java, python, sql]   |\n|gopi  |python,dotnet,dbms|[python, dotnet, dbms]|\n+------+------------------+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#split is used to split the string into array of values by using particular pattern\n",
    "data1=[('lokesh','java,python,sql'),('gopi','python,dotnet,dbms')]\n",
    "schema=['name','skills']\n",
    "df=spark.createDataFrame(data1,schema)\n",
    "df=df.withColumn('skills in array',split(col('skills'),','))\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a85edee-9eb5-46e0-bc67-e0b261ff8f60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----------+\n|  name|              skills|has skill?|\n+------+--------------------+----------+\n|lokesh| [java, python, sql]|      true|\n|  gopi|[python, dotnet, ...|      true|\n+------+--------------------+----------+\n\n+------+--------------------+----------+\n|  name|              skills|has skill?|\n+------+--------------------+----------+\n|lokesh| [java, python, sql]|      true|\n|  gopi|[python, dotnet, ...|     false|\n+------+--------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#array_contains is used to check for particular value present in array or not\n",
    "data=[('lokesh',['java','python','sql']),('gopi',['python','dotnet','dbms'])]\n",
    "schema=['name','skills']\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.withColumn('has skill?',array_contains(col('skills'),'python')).show()\n",
    "df.withColumn('has skill?',array_contains(col('skills'),'sql')).show()\n",
    "\n",
    "#df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2154d107-e172-43ae-a703-04ceb68afd8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#MAP type\n",
    "# MAP type is same as dictionary type\n",
    "from pyspark.sql.types import MapType,StringType,IntegerType,StructType,StructField\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5acee92a-bc0a-4758-8c11-348073ce6cc3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>marks</th></tr></thead><tbody><tr><td>lokesh</td><td>Map(english -> 40, telugu -> 25)</td></tr><tr><td>gopi</td><td>Map(english -> 48, telugu -> 65)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "lokesh",
         {
          "english": 40,
          "telugu": 25
         }
        ],
        [
         "gopi",
         {
          "english": 48,
          "telugu": 65
         }
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "marks",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"integer\",\"valueContainsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- marks: map (nullable = true)\n |    |-- key: string\n |    |-- value: integer (valueContainsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "data=[('lokesh',{'telugu':25,'english':40}),('gopi',{'telugu':65,'english':48})]\n",
    "schema=StructType([\\\n",
    "    StructField('name',StringType()),\\\n",
    "    StructField('marks',MapType(StringType(),IntegerType()))])\n",
    "dfMAP=spark.createDataFrame(data,schema)\n",
    "display(dfMAP)\n",
    "dfMAP.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51f1f89c-0fc7-4b91-a56e-add19c222dd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------+------+-------+\n|name  |marks                        |Telugu|english|\n+------+-----------------------------+------+-------+\n|lokesh|{english -> 40, telugu -> 25}|25    |40     |\n|gopi  |{english -> 48, telugu -> 65}|65    |48     |\n+------+-----------------------------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# accessing values of a map type\n",
    "dfMAP=dfMAP.withColumn('Telugu',col('marks')['telugu']).withColumn('english',col('marks')['english'])\n",
    "dfMAP.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b54b2a4-f4c0-4db0-8367-d8427b2304b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# functons used in map\n",
    "#     explode\n",
    "#     map_keys\n",
    "#     map_values\n",
    "from pyspark.sql.functions import explode,map_keys,map_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc26180e-fb57-4192-9ce9-7eaf8d910d68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------+-----+\n|  name|               marks|    key|value|\n+------+--------------------+-------+-----+\n|lokesh|{english -> 40, t...|english|   40|\n|lokesh|{english -> 40, t...| telugu|   25|\n|  gopi|{english -> 48, t...|english|   48|\n|  gopi|{english -> 48, t...| telugu|   65|\n+------+--------------------+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#explode\n",
    "dfMAP.select('name','marks',explode(col('marks'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28c6e9c0-0dc2-4525-b8ae-e064ed7e5268",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-----------------+-----------------+\n|  name|               marks|  map_keys(marks)|map_values(marks)|\n+------+--------------------+-----------------+-----------------+\n|lokesh|{english -> 40, t...|[english, telugu]|         [40, 25]|\n|  gopi|{english -> 48, t...|[english, telugu]|         [48, 65]|\n+------+--------------------+-----------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#map_keys & map_values\n",
    "dfMAP.select('name','marks',map_keys(col('marks')),map_values(col('marks'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2179d52a-71f0-47ba-a939-b4290893f90e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------+-------+-----------------+\n|  name|               marks|Telugu|english|             keys|\n+------+--------------------+------+-------+-----------------+\n|lokesh|{english -> 40, t...|    25|     40|[english, telugu]|\n|  gopi|{english -> 48, t...|    65|     48|[english, telugu]|\n+------+--------------------+------+-------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "dfMAP.withColumn('keys',map_keys(col('marks'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6eb30dc-a173-4806-93d0-feb494b6b135",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark-2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
